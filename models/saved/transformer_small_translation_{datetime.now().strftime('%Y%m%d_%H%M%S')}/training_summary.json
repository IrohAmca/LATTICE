{
    "model_name": "transformer_small_translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    "epoch": 3,
    "performance": {
        "best_val_loss": 3.3434951305389404,
        "loss": 3.3434951305389404,
        "perplexity": 28.317928314208984,
        "token_accuracy": 28.30188679245283,
        "bleu_score": 3.7477767366779213,
        "f1": 0.0,
        "precision": 0.0,
        "recall": 0.0
    },
    "architecture": {
        "total_parameters": 938785,
        "parameter_count": {
            "src_embed.0.lut.weight": 4608,
            "tgt_embed.0.lut.weight": 4224,
            "encoder.0.attn.linears.0.weight": 16384,
            "encoder.0.attn.linears.0.bias": 128,
            "encoder.0.attn.linears.1.weight": 16384,
            "encoder.0.attn.linears.1.bias": 128,
            "encoder.0.attn.linears.2.weight": 16384,
            "encoder.0.attn.linears.2.bias": 128,
            "encoder.0.attn.linears.3.weight": 16384,
            "encoder.0.attn.linears.3.bias": 128,
            "encoder.0.ffn.linear1.weight": 65536,
            "encoder.0.ffn.linear1.bias": 512,
            "encoder.0.ffn.linear2.weight": 65536,
            "encoder.0.ffn.linear2.bias": 128,
            "encoder.0.norm1.weight": 128,
            "encoder.0.norm1.bias": 128,
            "encoder.0.norm2.weight": 128,
            "encoder.0.norm2.bias": 128,
            "encoder.1.attn.linears.0.weight": 16384,
            "encoder.1.attn.linears.0.bias": 128,
            "encoder.1.attn.linears.1.weight": 16384,
            "encoder.1.attn.linears.1.bias": 128,
            "encoder.1.attn.linears.2.weight": 16384,
            "encoder.1.attn.linears.2.bias": 128,
            "encoder.1.attn.linears.3.weight": 16384,
            "encoder.1.attn.linears.3.bias": 128,
            "encoder.1.ffn.linear1.weight": 65536,
            "encoder.1.ffn.linear1.bias": 512,
            "encoder.1.ffn.linear2.weight": 65536,
            "encoder.1.ffn.linear2.bias": 128,
            "encoder.1.norm1.weight": 128,
            "encoder.1.norm1.bias": 128,
            "encoder.1.norm2.weight": 128,
            "encoder.1.norm2.bias": 128,
            "decoder.0.self_attn.linears.0.weight": 16384,
            "decoder.0.self_attn.linears.0.bias": 128,
            "decoder.0.self_attn.linears.1.weight": 16384,
            "decoder.0.self_attn.linears.1.bias": 128,
            "decoder.0.self_attn.linears.2.weight": 16384,
            "decoder.0.self_attn.linears.2.bias": 128,
            "decoder.0.self_attn.linears.3.weight": 16384,
            "decoder.0.self_attn.linears.3.bias": 128,
            "decoder.0.src_attn.linears.0.weight": 16384,
            "decoder.0.src_attn.linears.0.bias": 128,
            "decoder.0.src_attn.linears.1.weight": 16384,
            "decoder.0.src_attn.linears.1.bias": 128,
            "decoder.0.src_attn.linears.2.weight": 16384,
            "decoder.0.src_attn.linears.2.bias": 128,
            "decoder.0.src_attn.linears.3.weight": 16384,
            "decoder.0.src_attn.linears.3.bias": 128,
            "decoder.0.ffn.linear1.weight": 65536,
            "decoder.0.ffn.linear1.bias": 512,
            "decoder.0.ffn.linear2.weight": 65536,
            "decoder.0.ffn.linear2.bias": 128,
            "decoder.0.norm1.weight": 128,
            "decoder.0.norm1.bias": 128,
            "decoder.0.norm2.weight": 128,
            "decoder.0.norm2.bias": 128,
            "decoder.0.norm3.weight": 128,
            "decoder.0.norm3.bias": 128,
            "decoder.1.self_attn.linears.0.weight": 16384,
            "decoder.1.self_attn.linears.0.bias": 128,
            "decoder.1.self_attn.linears.1.weight": 16384,
            "decoder.1.self_attn.linears.1.bias": 128,
            "decoder.1.self_attn.linears.2.weight": 16384,
            "decoder.1.self_attn.linears.2.bias": 128,
            "decoder.1.self_attn.linears.3.weight": 16384,
            "decoder.1.self_attn.linears.3.bias": 128,
            "decoder.1.src_attn.linears.0.weight": 16384,
            "decoder.1.src_attn.linears.0.bias": 128,
            "decoder.1.src_attn.linears.1.weight": 16384,
            "decoder.1.src_attn.linears.1.bias": 128,
            "decoder.1.src_attn.linears.2.weight": 16384,
            "decoder.1.src_attn.linears.2.bias": 128,
            "decoder.1.src_attn.linears.3.weight": 16384,
            "decoder.1.src_attn.linears.3.bias": 128,
            "decoder.1.ffn.linear1.weight": 65536,
            "decoder.1.ffn.linear1.bias": 512,
            "decoder.1.ffn.linear2.weight": 65536,
            "decoder.1.ffn.linear2.bias": 128,
            "decoder.1.norm1.weight": 128,
            "decoder.1.norm1.bias": 128,
            "decoder.1.norm2.weight": 128,
            "decoder.1.norm2.bias": 128,
            "decoder.1.norm3.weight": 128,
            "decoder.1.norm3.bias": 128,
            "generator.linear.weight": 4224,
            "generator.linear.bias": 33
        },
        "model_params": {
            "src_vocab_size": 36,
            "tgt_vocab_size": 33,
            "encoder_layers": 2,
            "decoder_layers": 2,
            "d_model": 128,
            "d_ff": 512,
            "encoder_heads": 4,
            "decoder_heads": 4,
            "dropout": 0.15,
            "max_len": 32
        },
        "structure": "Transformer(\n  (src_embed): Sequential(\n    (0): Embeddings(\n      (lut): Embedding(36, 128)\n    )\n    (1): PositionalEncoding(\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (tgt_embed): Sequential(\n    (0): Embeddings(\n      (lut): Embedding(33, 128)\n    )\n    (1): PositionalEncoding(\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (encoder): ModuleList(\n    (0-1): 2 x EncoderTransformerBlock(\n      (attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n        )\n        (dropout): Dropout(p=0.15, inplace=False)\n      )\n      (ffn): PositionWiseFeedForward(\n        (linear1): Linear(in_features=128, out_features=512, bias=True)\n        (dropout): Dropout(p=0.15, inplace=False)\n        (linear2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (decoder): ModuleList(\n    (0-1): 2 x DecoderTransformerBlock(\n      (self_attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n        )\n        (dropout): Dropout(p=0.15, inplace=False)\n      )\n      (src_attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n        )\n        (dropout): Dropout(p=0.15, inplace=False)\n      )\n      (ffn): PositionWiseFeedForward(\n        (linear1): Linear(in_features=128, out_features=512, bias=True)\n        (dropout): Dropout(p=0.15, inplace=False)\n        (linear2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (generator): Generator(\n    (linear): Linear(in_features=128, out_features=33, bias=True)\n    (softmax): Softmax(dim=-1)\n  )\n)"
    },
    "training_config": {
        "early_stopping": {
            "enabled": true,
            "patience": 7,
            "min_delta": 0.005
        },
        "save_dir": "models/saved",
        "save_strategy": "epoch",
        "save_total_limit": 5,
        "dataset_name": "multi30k",
        "encoder_layers": 2,
        "decoder_layers": 2,
        "d_model": 128,
        "d_ff": 512,
        "encoder_heads": 4,
        "decoder_heads": 4,
        "max_len": 32,
        "epochs": 3,
        "max_samples": 20000,
        "min_freq": 2,
        "model_name": "transformer_small_translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "batch_size": 16,
        "learning_rate": 0.0003,
        "weight_decay": 0.1,
        "label_smoothing": 0.1,
        "warmup_ratio": 0.06,
        "max_grad_norm": 0.5,
        "gradient_accumulation_steps": 4,
        "use_curriculum_learning": true,
        "mixed_precision": true,
        "dropout": 0.15,
        "model_size": "small",
        "current_max_len": 3.2
    },
    "history": {
        "epochs": [
            1,
            2,
            3
        ],
        "train_loss": [
            3.497498261928558,
            3.4373775243759157,
            3.3896153211593627
        ],
        "train_acc": [
            0.0,
            15.09433962264151,
            28.30188679245283
        ],
        "val_loss": [
            3.501474618911743,
            3.401801109313965,
            3.3434951305389404
        ],
        "val_acc": [
            33.16432189941406,
            30.018115997314453,
            28.317928314208984
        ],
        "f1": [
            0.0,
            0.0,
            0.0
        ],
        "precision": [
            0.0,
            0.0,
            0.0
        ],
        "recall": [
            0.0,
            0.0,
            0.0
        ]
    },
    "metadata": {
        "timestamp": "2025-06-27 17:59:09",
        "model_name": "transformer_small_translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "tokenizer": "<class 'NoneType'>",
        "python_version": "3.12.3",
        "torch_version": "2.5.1+cu121",
        "cuda_version": "12.1",
        "system_info": "Linux-6.8.0-59-generic-x86_64-with-glibc2.39"
    }
}