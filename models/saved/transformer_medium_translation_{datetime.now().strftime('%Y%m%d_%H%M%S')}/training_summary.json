{
    "model_name": "transformer_medium_translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
    "epoch": 2,
    "performance": {
        "best_val_loss": 3.40995717048645,
        "loss": 3.40995717048645,
        "perplexity": 30.263948440551758,
        "token_accuracy": 15.09433962264151,
        "bleu_score": 1.727223799216787,
        "f1": 0.0,
        "precision": 0.0,
        "recall": 0.0
    },
    "architecture": {
        "total_parameters": 7398945,
        "parameter_count": {
            "src_embed.0.lut.weight": 9216,
            "tgt_embed.0.lut.weight": 8448,
            "encoder.0.attn.linears.0.weight": 65536,
            "encoder.0.attn.linears.0.bias": 256,
            "encoder.0.attn.linears.1.weight": 65536,
            "encoder.0.attn.linears.1.bias": 256,
            "encoder.0.attn.linears.2.weight": 65536,
            "encoder.0.attn.linears.2.bias": 256,
            "encoder.0.attn.linears.3.weight": 65536,
            "encoder.0.attn.linears.3.bias": 256,
            "encoder.0.ffn.linear1.weight": 262144,
            "encoder.0.ffn.linear1.bias": 1024,
            "encoder.0.ffn.linear2.weight": 262144,
            "encoder.0.ffn.linear2.bias": 256,
            "encoder.0.norm1.weight": 256,
            "encoder.0.norm1.bias": 256,
            "encoder.0.norm2.weight": 256,
            "encoder.0.norm2.bias": 256,
            "encoder.1.attn.linears.0.weight": 65536,
            "encoder.1.attn.linears.0.bias": 256,
            "encoder.1.attn.linears.1.weight": 65536,
            "encoder.1.attn.linears.1.bias": 256,
            "encoder.1.attn.linears.2.weight": 65536,
            "encoder.1.attn.linears.2.bias": 256,
            "encoder.1.attn.linears.3.weight": 65536,
            "encoder.1.attn.linears.3.bias": 256,
            "encoder.1.ffn.linear1.weight": 262144,
            "encoder.1.ffn.linear1.bias": 1024,
            "encoder.1.ffn.linear2.weight": 262144,
            "encoder.1.ffn.linear2.bias": 256,
            "encoder.1.norm1.weight": 256,
            "encoder.1.norm1.bias": 256,
            "encoder.1.norm2.weight": 256,
            "encoder.1.norm2.bias": 256,
            "encoder.2.attn.linears.0.weight": 65536,
            "encoder.2.attn.linears.0.bias": 256,
            "encoder.2.attn.linears.1.weight": 65536,
            "encoder.2.attn.linears.1.bias": 256,
            "encoder.2.attn.linears.2.weight": 65536,
            "encoder.2.attn.linears.2.bias": 256,
            "encoder.2.attn.linears.3.weight": 65536,
            "encoder.2.attn.linears.3.bias": 256,
            "encoder.2.ffn.linear1.weight": 262144,
            "encoder.2.ffn.linear1.bias": 1024,
            "encoder.2.ffn.linear2.weight": 262144,
            "encoder.2.ffn.linear2.bias": 256,
            "encoder.2.norm1.weight": 256,
            "encoder.2.norm1.bias": 256,
            "encoder.2.norm2.weight": 256,
            "encoder.2.norm2.bias": 256,
            "encoder.3.attn.linears.0.weight": 65536,
            "encoder.3.attn.linears.0.bias": 256,
            "encoder.3.attn.linears.1.weight": 65536,
            "encoder.3.attn.linears.1.bias": 256,
            "encoder.3.attn.linears.2.weight": 65536,
            "encoder.3.attn.linears.2.bias": 256,
            "encoder.3.attn.linears.3.weight": 65536,
            "encoder.3.attn.linears.3.bias": 256,
            "encoder.3.ffn.linear1.weight": 262144,
            "encoder.3.ffn.linear1.bias": 1024,
            "encoder.3.ffn.linear2.weight": 262144,
            "encoder.3.ffn.linear2.bias": 256,
            "encoder.3.norm1.weight": 256,
            "encoder.3.norm1.bias": 256,
            "encoder.3.norm2.weight": 256,
            "encoder.3.norm2.bias": 256,
            "decoder.0.self_attn.linears.0.weight": 65536,
            "decoder.0.self_attn.linears.0.bias": 256,
            "decoder.0.self_attn.linears.1.weight": 65536,
            "decoder.0.self_attn.linears.1.bias": 256,
            "decoder.0.self_attn.linears.2.weight": 65536,
            "decoder.0.self_attn.linears.2.bias": 256,
            "decoder.0.self_attn.linears.3.weight": 65536,
            "decoder.0.self_attn.linears.3.bias": 256,
            "decoder.0.src_attn.linears.0.weight": 65536,
            "decoder.0.src_attn.linears.0.bias": 256,
            "decoder.0.src_attn.linears.1.weight": 65536,
            "decoder.0.src_attn.linears.1.bias": 256,
            "decoder.0.src_attn.linears.2.weight": 65536,
            "decoder.0.src_attn.linears.2.bias": 256,
            "decoder.0.src_attn.linears.3.weight": 65536,
            "decoder.0.src_attn.linears.3.bias": 256,
            "decoder.0.ffn.linear1.weight": 262144,
            "decoder.0.ffn.linear1.bias": 1024,
            "decoder.0.ffn.linear2.weight": 262144,
            "decoder.0.ffn.linear2.bias": 256,
            "decoder.0.norm1.weight": 256,
            "decoder.0.norm1.bias": 256,
            "decoder.0.norm2.weight": 256,
            "decoder.0.norm2.bias": 256,
            "decoder.0.norm3.weight": 256,
            "decoder.0.norm3.bias": 256,
            "decoder.1.self_attn.linears.0.weight": 65536,
            "decoder.1.self_attn.linears.0.bias": 256,
            "decoder.1.self_attn.linears.1.weight": 65536,
            "decoder.1.self_attn.linears.1.bias": 256,
            "decoder.1.self_attn.linears.2.weight": 65536,
            "decoder.1.self_attn.linears.2.bias": 256,
            "decoder.1.self_attn.linears.3.weight": 65536,
            "decoder.1.self_attn.linears.3.bias": 256,
            "decoder.1.src_attn.linears.0.weight": 65536,
            "decoder.1.src_attn.linears.0.bias": 256,
            "decoder.1.src_attn.linears.1.weight": 65536,
            "decoder.1.src_attn.linears.1.bias": 256,
            "decoder.1.src_attn.linears.2.weight": 65536,
            "decoder.1.src_attn.linears.2.bias": 256,
            "decoder.1.src_attn.linears.3.weight": 65536,
            "decoder.1.src_attn.linears.3.bias": 256,
            "decoder.1.ffn.linear1.weight": 262144,
            "decoder.1.ffn.linear1.bias": 1024,
            "decoder.1.ffn.linear2.weight": 262144,
            "decoder.1.ffn.linear2.bias": 256,
            "decoder.1.norm1.weight": 256,
            "decoder.1.norm1.bias": 256,
            "decoder.1.norm2.weight": 256,
            "decoder.1.norm2.bias": 256,
            "decoder.1.norm3.weight": 256,
            "decoder.1.norm3.bias": 256,
            "decoder.2.self_attn.linears.0.weight": 65536,
            "decoder.2.self_attn.linears.0.bias": 256,
            "decoder.2.self_attn.linears.1.weight": 65536,
            "decoder.2.self_attn.linears.1.bias": 256,
            "decoder.2.self_attn.linears.2.weight": 65536,
            "decoder.2.self_attn.linears.2.bias": 256,
            "decoder.2.self_attn.linears.3.weight": 65536,
            "decoder.2.self_attn.linears.3.bias": 256,
            "decoder.2.src_attn.linears.0.weight": 65536,
            "decoder.2.src_attn.linears.0.bias": 256,
            "decoder.2.src_attn.linears.1.weight": 65536,
            "decoder.2.src_attn.linears.1.bias": 256,
            "decoder.2.src_attn.linears.2.weight": 65536,
            "decoder.2.src_attn.linears.2.bias": 256,
            "decoder.2.src_attn.linears.3.weight": 65536,
            "decoder.2.src_attn.linears.3.bias": 256,
            "decoder.2.ffn.linear1.weight": 262144,
            "decoder.2.ffn.linear1.bias": 1024,
            "decoder.2.ffn.linear2.weight": 262144,
            "decoder.2.ffn.linear2.bias": 256,
            "decoder.2.norm1.weight": 256,
            "decoder.2.norm1.bias": 256,
            "decoder.2.norm2.weight": 256,
            "decoder.2.norm2.bias": 256,
            "decoder.2.norm3.weight": 256,
            "decoder.2.norm3.bias": 256,
            "decoder.3.self_attn.linears.0.weight": 65536,
            "decoder.3.self_attn.linears.0.bias": 256,
            "decoder.3.self_attn.linears.1.weight": 65536,
            "decoder.3.self_attn.linears.1.bias": 256,
            "decoder.3.self_attn.linears.2.weight": 65536,
            "decoder.3.self_attn.linears.2.bias": 256,
            "decoder.3.self_attn.linears.3.weight": 65536,
            "decoder.3.self_attn.linears.3.bias": 256,
            "decoder.3.src_attn.linears.0.weight": 65536,
            "decoder.3.src_attn.linears.0.bias": 256,
            "decoder.3.src_attn.linears.1.weight": 65536,
            "decoder.3.src_attn.linears.1.bias": 256,
            "decoder.3.src_attn.linears.2.weight": 65536,
            "decoder.3.src_attn.linears.2.bias": 256,
            "decoder.3.src_attn.linears.3.weight": 65536,
            "decoder.3.src_attn.linears.3.bias": 256,
            "decoder.3.ffn.linear1.weight": 262144,
            "decoder.3.ffn.linear1.bias": 1024,
            "decoder.3.ffn.linear2.weight": 262144,
            "decoder.3.ffn.linear2.bias": 256,
            "decoder.3.norm1.weight": 256,
            "decoder.3.norm1.bias": 256,
            "decoder.3.norm2.weight": 256,
            "decoder.3.norm2.bias": 256,
            "decoder.3.norm3.weight": 256,
            "decoder.3.norm3.bias": 256,
            "generator.linear.weight": 8448,
            "generator.linear.bias": 33
        },
        "model_params": {
            "src_vocab_size": 36,
            "tgt_vocab_size": 33,
            "encoder_layers": 4,
            "decoder_layers": 4,
            "d_model": 256,
            "d_ff": 1024,
            "encoder_heads": 8,
            "decoder_heads": 8,
            "dropout": 0.15,
            "max_len": 64
        },
        "structure": "Transformer(\n  (src_embed): Sequential(\n    (0): Embeddings(\n      (lut): Embedding(36, 256)\n    )\n    (1): PositionalEncoding(\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (tgt_embed): Sequential(\n    (0): Embeddings(\n      (lut): Embedding(33, 256)\n    )\n    (1): PositionalEncoding(\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (encoder): ModuleList(\n    (0-3): 4 x EncoderTransformerBlock(\n      (attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n        )\n        (dropout): Dropout(p=0.15, inplace=False)\n      )\n      (ffn): PositionWiseFeedForward(\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.15, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n      )\n      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (decoder): ModuleList(\n    (0-3): 4 x DecoderTransformerBlock(\n      (self_attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n        )\n        (dropout): Dropout(p=0.15, inplace=False)\n      )\n      (src_attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n        )\n        (dropout): Dropout(p=0.15, inplace=False)\n      )\n      (ffn): PositionWiseFeedForward(\n        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n        (dropout): Dropout(p=0.15, inplace=False)\n        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n      )\n      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.15, inplace=False)\n    )\n  )\n  (generator): Generator(\n    (linear): Linear(in_features=256, out_features=33, bias=True)\n    (softmax): Softmax(dim=-1)\n  )\n)"
    },
    "training_config": {
        "learning_rate": 0.0003,
        "weight_decay": 0.1,
        "warmup_ratio": 0.06,
        "max_grad_norm": 0.5,
        "gradient_accumulation_steps": 4,
        "mixed_precision": true,
        "early_stopping": {
            "enabled": true,
            "patience": 7,
            "min_delta": 0.005
        },
        "use_curriculum_learning": true,
        "save_dir": "models/saved",
        "save_strategy": "epoch",
        "save_total_limit": 5,
        "dataset_name": "multi30k",
        "dropout": 0.15,
        "encoder_layers": 4,
        "decoder_layers": 4,
        "d_model": 256,
        "d_ff": 1024,
        "encoder_heads": 8,
        "decoder_heads": 8,
        "max_len": 64,
        "epochs": 10,
        "max_samples": 100000,
        "min_freq": 3,
        "model_name": "transformer_medium_translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "batch_size": 32,
        "model_size": "medium",
        "current_max_len": 1.6
    },
    "history": {
        "epochs": [
            1,
            2
        ],
        "train_loss": [
            3.4597043991088867,
            3.4291942834854128
        ],
        "train_acc": [
            13.20754716981132,
            15.09433962264151
        ],
        "val_loss": [
            3.419604539871216,
            3.40995717048645
        ],
        "val_acc": [
            30.557329177856445,
            30.263948440551758
        ],
        "f1": [
            0.0,
            0.0
        ],
        "precision": [
            0.0,
            0.0
        ],
        "recall": [
            0.0,
            0.0
        ]
    },
    "metadata": {
        "timestamp": "2025-06-27 16:21:08",
        "model_name": "transformer_medium_translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "tokenizer": "<class 'NoneType'>",
        "python_version": "3.12.3",
        "torch_version": "2.5.1+cu121",
        "cuda_version": "12.1",
        "system_info": "Linux-6.8.0-59-generic-x86_64-with-glibc2.39"
    }
}